# 분류
선형 회귀를 이용해 분류를 할 수 있지만 예외적인 데이터에 최적선이 큰 영향을 받아 분류로는 적합하지 않음

# decision boundary
데이터를 분류하는 결정 경계선, 변수가 많아질수록 시각적 표현 어려움

# 로지스틱 회귀
1. 시그모이드 함수 : S(x) = {1 + e^(-x)}^(-1), 0과 1사이릐 값을 리턴
2. 가설 함수 : h(x) = {1 + e^(-x * theta.T)}^(-1)

# 로그 손실(log-loss/ cross entropy)
분류는 0 or 1이므로 실제 y의 값에 따라 손실 함수가 다른 모양
1. y==1 경우 : -log(h(x))  
2. y==0 경우 : -log(1 - h(x))
3. 일반화(모든 경우) : logloss(h(x), y) = -ylog(h(x)) - (1 - y)log(1 - h(x))

# 경사 하강법
선형 회귀와 거의 동일

import numpy as np

def sigmoid(x):
    """시그모이드 함수"""
    return 1 / (1 + np.exp(-x))
    
    
def prediction(X, theta):
    """로지스틱 회귀 가정 함수"""
    return sigmoid(X @ theta)
    

def gradient_descent(X, theta, y, iterations, alpha):
    """로지스틱 회귀 경사 하강 알고리즘"""
    m = len(X)  # 입력 변수 개수 저장

    for _ in range(iterations):
        error = prediction(X, theta) - y
        theta = theta - alpha / m * (X.T @ error)
            
    return theta

# scikit-learn, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=5)
y_tr = y_tr.values.ravel(). #선형회귀와 다름, 경고를 피하기위해
model = LogisticRegression(solver='saga', max_iter=2000)
        - solver : 어떤 알고리즘을 사용할지 결정
          max_iter : 몇번 반복으로 학습할지 결정

model.fit(X_tr, y_tr)
model.predict(X_te)
model.score(X_te, y_te) : 분류는 직관적으로 평가할 수 있다



