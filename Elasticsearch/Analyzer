//텍스트 분석
  엘라스틱서치는 루씬이 제공하는 분석기를 그대로 사용
  엘라스틱서치는 문서를 색인하기 전에 해당 문서의 필드 타입이 무엇인지 확인하고 text인 경우 분석
  이후 형태소 형태로 분석하고 특정 원칙에 의해 필터링되어 단어가 삭제, 추가, 수정 등의 과정등을 거쳐 역색인


//역색인 구조
  1. 모든 문서가 가지는 단어의 고유 단어 목록
  2. 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
  3. 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
  4. 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도

  
//토큰화
  문서를 역색인을 만들기 위해 각 문서를 토큰화 필수
  토큰화된 단어에 대해 문서 상의 위치와 출현빈도 등의 정보를 체크
  색인한다는 것은 역색인 파일을 만든다는 것

    //문서
    문서1: elasticsearch is cool
    문서2: Elasticsearch is great

    //토큰정보(토큰, 문서번호, 텀의 위치, 텀의 빈도) - 실제로는 더 많은 정보 저장
    elasticsearch  | 문서1         | 1     | 1
    Elasticsearch  | 문서2         | 1     | 1
    is             | 문서1, 문서2   | 2, 2  | 2
    cool           | 문서1         | 3     | 1
    great          | 문서2         | 3     | 1


//분석기의 구조
  1. 문장을 특정한 규칙에 의해 수정(CHARACTER FILTER)
    문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거
    개별 토큰화하기 전의 전처리 과정
    패턴으로 텍스트를 변경하거나 사용자 정의 필터 사용 가능

  2. 수정한 문장을 개별 토큰으로 분리(TOKENIZER FILTER)
    분석기를 구성할 때 하나만 사용할 수 있으며 텍스트를 어떻게 나눌 것인지 정의
    보통 언어에 맞는 Tokenizer를 사용(한글, 영문 등)

  3. 개별 토큰을 특정한 규칙에 의해 변경(TOKEN FILTER)
    토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환
    불필요한 단어 제거
    동의어 사전 생성 및 단어 추가
    영문 소문자 변환
    여러 단계가 순차적으로 이뤄지며 순서에 따라 검색의 질이 상이


//사용자 정의 분석기 생성
  char_filter: CHARACTER FILTER 지정, 배열로 여러개 지정 가능
  tokenizer: TOKENIZER FILTER, 하나만 지정 가능
  filter: TOKEN FILTER, 배열로 여러개 지정 가능

    PUT /movie_analyzer
    {
      "settings": {
        "index": {
          "number_of_shards": 5,
          "number_of_replicas": 1
        }
      },
      "analysis": {
        "analyzers": {
          "custom_movie_analyzer": {
            "type": "custom",
            "char_filter": [
              "html_strip"
            ],
            "tokenizer": "standard",
            "filter": [
              "lowercase"
            ]
          }
        }
      }
    }


//분석기 사용법
  분석기 사용을 위한 _analyzer API 제공

  1. 분석기를 이용한 분석
    현태소 분석을 위한 _analyze API

      POST _analyze
      {
        "analyzer": "standard",
        "text": "캐리비안의 해적"
      }
    
  2. 필드를 이용한 분석
    인덱스 설정시 분석기를 직접 설정 가능
    매핑 설정에 title 필드를 custom_movie_analyzer에 매핑한 경우 아래와 같이 사용가능

      POST movie_analyzer/_analyze
      {
        "field": "title",
        "text": "캐리비안의 해적"
      }

  3. 색인과 검색시 분석기 각각 설정
    Index Analyzer와 Search Analyzer로 구분해 구성 가능
    필드에 원하는 분석기를 지정 가능

      PUT movie_analyzer
      {
        "settings": {
          "index": {
            "number_of_shards": 5,
            "number_of_replicas": 1
          }
        },
        "analysis": {
          "analyzer": {
            "movie_lower_test_analyzer": {
              "type": "custom",
              "tokenizer": "standard",
              "filter": [
                "lowercase"
              ]
            },
            "movie_stop_test_analyzer": {
              "type": "custom",
              "tokenizer": "standard",
              "filter": [
                "lowercase",
                "english_stop"
              ]
            }
          },
          "filter": {
            "english_stop": {
              "type": "stop",
              "stopwords": "_english_"
            }
          }
        },
        "mappings": {
          "_doc": {
            "properties": {
              "title": {
                "type": "text",
                "analyzer": "movie_stop_test_analyzer",
                "search_analyzer": "movie_lower_test_analyzer"
              }
            }
          }
        }
      }


//대표적인 분석기
  1. Standard Analyzer

      Tokenizer: Standard Tokenizer
      Token Filter: Standard Token Filter, Lower Case Token Filter

    아무런 정의를 하지 않고 필드 데이터 타입을 text로 지정한 경우 사용되는 분석기
    공백 혹은 특수 기호를 기준으로 토큰 분리
    모든 문자를 소문자로 변경
    max_token_length: 최대 토큰 길이를 초과하는 토큰이 있는 경우 설정한 값으로 분할, 기본값 255
    stopwords: 사전 정의된 불용어 사전을 사용, 기본값은 사용하지 않음
    stopwords_path: 불용어가 포함된 파일을 사용할 경우 서버의 경로로 사용


  2. Whitespace Analyzer

      Tokenizer: Whitespace Tokenizer
      Token Filter: 없음

    공백 문자열 기준으로 토큰 분리하는 간단한 분석기


  3. Keyword Analyzer

      Tokenizer: Keyword Tokenizer
      Token Filter: 없음

    전체 입력 문자열을 하나의 키워드처럼 처리
    토큰화 작업을 하지 않음
      



